{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim of this lab is to develop a Language Model based on the log linear model.\n",
    "\n",
    "Again, we use the data from `data/iwslt-en-de-preprocessed.tar.gz`. The dataset we are going to use comes from German-English translation task in IWSLT campaign."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import re\n",
    "\n",
    "from math import log\n",
    "from collections import Counter, defaultdict\n",
    "from itertools import chain\n",
    "\n",
    "import numpy as np\n",
    "import pandas\n",
    "from scipy.special import softmax\n",
    "\n",
    "from processdata import get_vocabs, tokenize, START_SYMBOL, STOP_SYMBOL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCABS = get_vocabs(\"data/ngram/train.txt\", \"data/ngram/valid.txt\", \"data/ngram/test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_mapping_from_vocabs(vocab):\n",
    "  with open('data/loglinear/vocab_mapping.txt', 'w') as f:\n",
    "    for i, v in enumerate(vocab):\n",
    "      f.write(f\"{v}\\t{i}\\n\")\n",
    "\n",
    "generate_mapping_from_vocabs(VOCABS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mapping_from_vocabs(file, convert_to_int=True):\n",
    "  mapping = {}\n",
    "  with open(file, 'r') as f:\n",
    "    for l in f:\n",
    "      k, v = l.rstrip().split('\\t')\n",
    "      if convert_to_int:\n",
    "        mapping[k] = int(v)\n",
    "      else:\n",
    "        mapping[k] = v\n",
    "  return mapping\n",
    "\n",
    "def transform_data(mapping, file):\n",
    "  seqs = []\n",
    "  with open(file, 'r') as f:\n",
    "    for l in f:\n",
    "      line_mapping = [mapping[t] for t in tokenize(l)]\n",
    "      seqs.append(line_mapping)\n",
    "  \n",
    "  return seqs\n",
    "\n",
    "def writing_seq_idx(filename, seqs):\n",
    "  with open(filename, 'w') as f:\n",
    "    for l in seqs:\n",
    "      f.write(\" \".join(l) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = load_mapping_from_vocabs('data/loglinear/vocab_mapping.txt', convert_to_int=False)\n",
    "\n",
    "for i in [\"train\", \"valid\", \"test\"]:\n",
    "  writing_seq_idx(f\"data/loglinear/{i}.txt\", transform_data(mapping, f\"data/ngram/{i}.txt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Design feature functions\n",
    "\n",
    "Let's just use one-hot vector.\n",
    "How many features do we need?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanilla feature functions\n",
    "\n",
    "Here we go. How large it is to represent all features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_fn(seq, vocab_size):\n",
    "  def gen():\n",
    "    for s in seq:\n",
    "      feature = np.zeros(vocab_size, dtype=np.float32)\n",
    "      feature[s] = 1\n",
    "      yield feature\n",
    "  \n",
    "  return np.vstack([f for f in gen()]).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# play with some parameters\n",
    "vocab_size = 5\n",
    "feature_length = 2\n",
    "seq = (3,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 1. 0. 0. 0. 1. 0. 0.] (10,)\n"
     ]
    }
   ],
   "source": [
    "print(feature_fn(seq, vocab_size), feature_fn(seq, vocab_size).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score function and loss function\n",
    "Now we need to set up the computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(vocab_size, feature_length):\n",
    "  W = np.empty((vocab_size, feature_length*vocab_size), dtype=np.float32)\n",
    "  b = np.empty((vocab_size), dtype=np.float32)\n",
    "  return W, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "W, b = build_model(vocab_size, feature_length) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(x, W, b):\n",
    "  return np.dot(W, x) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.5249703e-29 2.0938583e-38 1.3213755e-19 1.0000000e+00 1.9836774e-38]\n"
     ]
    }
   ],
   "source": [
    "# try it\n",
    "x = feature_fn(seq, vocab_size)\n",
    "s = score(x, W, b)\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probability(s):\n",
    "  res = softmax(s)\n",
    "  res[res==0.0] = 1e-45 # a quick fix on clipping\n",
    "  return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.14884758 0.14884758 0.14884758 0.40460965 0.14884758]\n"
     ]
    }
   ],
   "source": [
    "p = probability(s)\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(estimated, expected):\n",
    "  mask = expected>=1.0\n",
    "  return sum(-np.ma.log(estimated[mask]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.9048324823379517"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = 2\n",
    "target_onehot = feature_fn((target,), vocab_size)\n",
    "loss(p, target_onehot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### calculate derivatives\n",
    "\n",
    "Firstly, the compuation is here:\n",
    "\n",
    "$$\n",
    "\\begin{aligned} \\boldsymbol{x} &=\\phi\\left(e_{t-m+1}^{t-1}\\right) \\\\ \\boldsymbol{s} &=\\sum_{\\left\\{j : x_{j} !=0\\right\\}} W _{:,j} x_{j}+\\boldsymbol{b} \\\\ \\boldsymbol{p} &=\\operatorname{softmax}(\\boldsymbol{s}) \\\\ \\ell &=-\\log \\boldsymbol{p}_{e_{t}} \\end{aligned}\n",
    "$$\n",
    "\n",
    "We can get:\n",
    "\n",
    "$$\n",
    "\\begin{aligned} \\frac{d \\ell\\left(e_{t-n+1}^{t}, W, \\boldsymbol{b}\\right)}{d \\boldsymbol{b}} &=\\boldsymbol{p}-\\text { onehot }\\left(e_{t}\\right) \\\\ \\frac{d \\ell\\left(e_{t-n+1}^{t}, W, \\boldsymbol{b}\\right)}{d W_{\\cdot, j}} &=x_{j}\\left(\\boldsymbol{p}-\\text { onehot }\\left(e_{t}\\right)\\right) \\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.14884758  0.14884758 -0.8511524   0.40460965  0.14884758] (5,) (5,)\n",
      "[[ 0.          0.          0.          0.14884758  0.          0.\n",
      "   0.          0.14884758  0.          0.        ]\n",
      " [ 0.          0.          0.          0.14884758  0.          0.\n",
      "   0.          0.14884758  0.          0.        ]\n",
      " [-0.         -0.         -0.         -0.85115242 -0.         -0.\n",
      "  -0.         -0.85115242 -0.         -0.        ]\n",
      " [ 0.          0.          0.          0.40460965  0.          0.\n",
      "   0.          0.40460965  0.          0.        ]\n",
      " [ 0.          0.          0.          0.14884758  0.          0.\n",
      "   0.          0.14884758  0.          0.        ]] (5, 10) (5, 10)\n"
     ]
    }
   ],
   "source": [
    "db = p - target_onehot\n",
    "dW = ((x >= 1.0).astype(int).reshape(-1, 1) * db).T\n",
    "print(db, b.shape, db.shape)\n",
    "print(dW, W.shape, dW.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd_opt_step(W, b, dW, db, lr=1e-4):\n",
    "  W -= lr * dW\n",
    "  b -= lr * db\n",
    "  return W, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 1. 0.]\n",
      "[-1.4884758e-05 -1.4884758e-05  8.5115236e-05  9.9995953e-01\n",
      " -1.4884758e-05]\n"
     ]
    }
   ],
   "source": [
    "# See how it changes\n",
    "print(b)\n",
    "W, b = sgd_opt_step(W, b, dW, db, lr=1e-4)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimize our computation\n",
    "\n",
    "In fact, we are already optimize the computation in optimization steps.\n",
    "The things for our model is that we don't need to generate a one-hot vector for a feature.\n",
    "We only needs to pluck a column instead of dot product when we perform transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def col_feature_fn(seq, vocab_size):\n",
    "  def gen():\n",
    "    acc = 0\n",
    "    for s in seq:\n",
    "      yield s + acc\n",
    "      acc += vocab_size\n",
    "  \n",
    "  return [f for f in gen()]\n",
    "\n",
    "def score(x, W, b):\n",
    "  # we don't have broadcast according to formula. It's different than engineering process\n",
    "  return np.sum(W[:,x], axis=1) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-4.4654273e-05, -4.4654273e-05,  2.5534572e-04,  9.9987859e-01,\n",
       "       -4.4654273e-05], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = col_feature_fn(seq, vocab_size)\n",
    "score(x, W, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting them together\n",
    "Just show case which part might be really slow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext line_profiler\n",
    "\n",
    "def step():\n",
    "  feature_length = 2\n",
    "  vocab_size = len(VOCABS)\n",
    "\n",
    "  seq = (1,2)\n",
    "  target = 3\n",
    "  W, b = build_model(vocab_size, feature_length) \n",
    "  x = col_feature_fn(seq, vocab_size)\n",
    "  s = score(x, W, b)\n",
    "  p = probability(s)\n",
    "  target_onehot = feature_fn((target,), vocab_size)\n",
    "  l = loss(p, target_onehot)\n",
    "  db = p - target_onehot\n",
    "  dW = np.zeros(W.shape[1], dtype=np.float32) # we spent a lot of time here though!\n",
    "  dW[np.array(x)] = 1.0\n",
    "  dW = (dW.reshape(-1, 1) * db).T\n",
    "  sgd_opt_step(W, b, dW, db, lr=1e-4)\n",
    "\n",
    "%lprun -f step step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequence(\n",
    "    sequence,\n",
    "    n,\n",
    "    mapping\n",
    "):\n",
    "  LEFT_PAD_SYMBOL = int(mapping['<s>'])\n",
    "  RIGHT_PAD_SYMBOL = int(mapping['</s>'])\n",
    "  return chain((LEFT_PAD_SYMBOL,) * (n - 1), iter(sequence), (RIGHT_PAD_SYMBOL,) * (n - 1))\n",
    "\n",
    "def ngram(sequence, n, mapping):\n",
    "  sequence = pad_sequence(sequence, n, mapping)\n",
    "\n",
    "  history = []\n",
    "  while n > 1:\n",
    "    try:\n",
    "      next_item = next(sequence)\n",
    "    except StopIteration:\n",
    "      # no more data, terminate the generator\n",
    "      return\n",
    "    history.append(next_item)\n",
    "    n -= 1\n",
    "  \n",
    "  for item in sequence:\n",
    "    history.append(mapping[item])\n",
    "    yield tuple(history)\n",
    "    del history[0]\n",
    "\n",
    "def train(filename, mapping, epoch=10):\n",
    "  vocab_size = len(VOCABS)\n",
    "  feature_length = 2\n",
    "  W, b = build_model(vocab_size, feature_length) \n",
    "  \n",
    "  with open(filename, 'r') as f:\n",
    "    for e in range(epoch):\n",
    "      dW = np.zeros(W.shape, np.float32)\n",
    "      db = np.zeros(b.shape, np.float32)\n",
    "      loss_val = 0.0\n",
    "      i = 0\n",
    "      \n",
    "      for l in f:\n",
    "        for seq in ngram(map(int, l.rstrip().split(' ')), feature_length+1, mapping):\n",
    "          x = col_feature_fn(seq[:-1], vocab_size)\n",
    "          s = score(x, W, b)\n",
    "          p = probability(s)\n",
    "\n",
    "          target_onehot = feature_fn((seq[-1],), vocab_size)\n",
    "          loss_val += loss(p, target_onehot)\n",
    "\n",
    "          db_val = p - target_onehot\n",
    "          db += db_val\n",
    "          dW[:,x] += (dW[:,x].T + db_val.T).T\n",
    "\n",
    "        i += 1\n",
    "        if i >= 100:\n",
    "          i = 0\n",
    "          print(f\"loss: {loss_val}\")\n",
    "          loss_val = 0.0\n",
    "          W, b = sgd_opt_step(W, b, dW, db, lr=1e-4)\n",
    "          dW = np.zeros(W.shape, np.float32)\n",
    "          db = np.zeros(b.shape, np.float32)\n",
    "      f.seek(0)\n",
    "  return W, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:51: RuntimeWarning: overflow encountered in add\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 23338.241806030273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/scipy/special/_logsumexp.py:215: RuntimeWarning: invalid value encountered in subtract\n",
      "  return np.exp(x - logsumexp(x, axis=axis, keepdims=True))\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: ''",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-9be9cbe0b67b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data/loglinear/train.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-38-c0e9158b4d73>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(filename, mapping, epoch)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mseq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mngram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_length\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m           \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcol_feature_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m           \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-38-c0e9158b4d73>\u001b[0m in \u001b[0;36mngram\u001b[0;34m(sequence, n, mapping)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mn\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msequence\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32myield\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: ''"
     ]
    }
   ],
   "source": [
    "W, b = train(\"data/loglinear/train.txt\", mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not efficient code at all. Since we take all data into generate vocabs,\n",
    "so we don't need to deal with unknown words. But if we want to make it more fruitful,\n",
    "we can generate vocabs based on training data (which is also correct).\n",
    "\n",
    "And be noted, this is not common practice to write computation like this.\n",
    "It's hard to vectorize. It's hard to use broadcast from numpy (which asks us to align the last dimension).\n",
    "Why is that? (Thing about memory layout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "navigate_menu": false,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
