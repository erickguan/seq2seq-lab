{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2-log-linear-model.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fantasticfears/seq2seq-lab/blob/master/2-log-linear-model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vh1Nt4WcdINT",
        "colab_type": "text"
      },
      "source": [
        "The aim of this lab is to develop a Language Model based on the log linear model.\n",
        "\n",
        "Again, we use the data from `data/iwslt-en-de-preprocessed.tar.gz`. The dataset we are going to use comes from German-English translation task in IWSLT campaign.\n",
        "\n",
        "I'm not in favor of this excises nor the symbols used in the tutorial."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zk4KR1pVdINW",
        "colab_type": "text"
      },
      "source": [
        "## Prepare data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AHpiHBfRdINY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tarfile\n",
        "import re\n",
        "\n",
        "from math import log\n",
        "from collections import Counter, defaultdict\n",
        "from itertools import chain\n",
        "\n",
        "import numpy as np\n",
        "import pandas\n",
        "from scipy.special import softmax\n",
        "\n",
        "from processdata import get_vocabs, tokenize, START_SYMBOL, STOP_SYMBOL"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3PZ_IOn8dINc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "VOCABS = get_vocabs(\"data/ngram/train.txt\", \"data/ngram/valid.txt\", \"data/ngram/test.txt\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "El1X-S11dINh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_mapping_from_vocabs(vocab):\n",
        "  with open('data/loglinear/vocab_mapping.txt', 'w') as f:\n",
        "    for i, v in enumerate(vocab):\n",
        "      f.write(f\"{v}\\t{i}\\n\")\n",
        "\n",
        "generate_mapping_from_vocabs(VOCABS)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kxAfbBFbdINk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_mapping_from_vocabs(file, convert_to_int=True):\n",
        "  mapping = {}\n",
        "  with open(file, 'r') as f:\n",
        "    for l in f:\n",
        "      k, v = l.rstrip().split('\\t')\n",
        "      if convert_to_int:\n",
        "        mapping[k] = int(v)\n",
        "      else:\n",
        "        mapping[k] = v\n",
        "  return mapping\n",
        "\n",
        "def transform_data(mapping, file):\n",
        "  seqs = []\n",
        "  with open(file, 'r') as f:\n",
        "    for l in f:\n",
        "      line_mapping = [mapping[t] for t in tokenize(l)]\n",
        "      seqs.append(line_mapping)\n",
        "  \n",
        "  return seqs\n",
        "\n",
        "def writing_seq_idx(filename, seqs):\n",
        "  with open(filename, 'w') as f:\n",
        "    for l in seqs:\n",
        "      f.write(\" \".join(l) + \"\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K7GDJjbBdINn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mapping = load_mapping_from_vocabs('data/loglinear/vocab_mapping.txt', convert_to_int=False)\n",
        "\n",
        "for i in [\"train\", \"valid\", \"test\"]:\n",
        "  writing_seq_idx(f\"data/loglinear/{i}.txt\", transform_data(mapping, f\"data/ngram/{i}.txt\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9J6gjuXodINq",
        "colab_type": "text"
      },
      "source": [
        "## Design feature functions\n",
        "\n",
        "Let's just use one-hot vector.\n",
        "How many features do we need?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SatpMu5cdINr",
        "colab_type": "text"
      },
      "source": [
        "### Vanilla feature functions\n",
        "\n",
        "Here we go. How large it is to represent all features?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "thODXH0ldINs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def feature_fn(seq, vocab_size):\n",
        "  def gen():\n",
        "    for s in seq:\n",
        "      feature = np.zeros(vocab_size, dtype=np.float32)\n",
        "      feature[s] = 1\n",
        "      yield feature\n",
        "  \n",
        "  return np.vstack([f for f in gen()]).reshape(-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mA1kYNAgdINv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# play with some parameters\n",
        "vocab_size = 5\n",
        "feature_length = 2\n",
        "seq = (3,2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4rEeemN9dINy",
        "colab_type": "code",
        "outputId": "3165516d-c7c5-45de-bf48-321eae323994",
        "colab": {}
      },
      "source": [
        "print(feature_fn(seq, vocab_size), feature_fn(seq, vocab_size).shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0. 0. 0. 1. 0. 0. 0. 1. 0. 0.] (10,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZX0t801dIPa",
        "colab_type": "text"
      },
      "source": [
        "### Score function and loss function\n",
        "Now we need to set up the computation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4CQ9pLtmdIPb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model(vocab_size, feature_length):\n",
        "  W = np.empty((vocab_size, feature_length*vocab_size), dtype=np.float32)\n",
        "  b = np.empty((vocab_size), dtype=np.float32)\n",
        "  return W, b"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EABULBxRdIPd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "W, b = build_model(vocab_size, feature_length) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iTpncuBGdIPf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def score(x, W, b):\n",
        "  return np.dot(W, x) + b"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZrcOMEPidIPh",
        "colab_type": "code",
        "outputId": "0bfb32fe-5f22-483d-c139-714fc3d59a6f",
        "colab": {}
      },
      "source": [
        "# try it\n",
        "x = feature_fn(seq, vocab_size)\n",
        "s = score(x, W, b)\n",
        "print(s)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2.5249703e-29 2.0938583e-38 1.3213755e-19 1.0000000e+00 1.9836774e-38]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TDFWFjQYdIPm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def probability(s):\n",
        "  res = softmax(s)\n",
        "  res[res==0.0] = 1e-45 # a quick fix on clipping\n",
        "  return res"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_LD9B6QdIPo",
        "colab_type": "code",
        "outputId": "f11d609e-4fef-44ee-87e3-91efa2f974e0",
        "colab": {}
      },
      "source": [
        "p = probability(s)\n",
        "print(p)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.14884758 0.14884758 0.14884758 0.40460965 0.14884758]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OcnokgsTdIPs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loss(estimated, expected):\n",
        "  mask = expected>=1.0\n",
        "  return sum(-np.ma.log(estimated[mask]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P3pz9E1fdIPu",
        "colab_type": "code",
        "outputId": "2282fcb5-4883-42fb-83de-a4e41c09a4f0",
        "colab": {}
      },
      "source": [
        "target = 2\n",
        "target_onehot = feature_fn((target,), vocab_size)\n",
        "loss(p, target_onehot)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.9048324823379517"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8raGCCTvdIPy",
        "colab_type": "text"
      },
      "source": [
        "### calculate derivatives\n",
        "\n",
        "Firstly, the compuation is here:\n",
        "\n",
        "$$\n",
        "\\begin{aligned} \\boldsymbol{x} &=\\phi\\left(e_{t-m+1}^{t-1}\\right) \\\\ \\boldsymbol{s} &=\\sum_{\\left\\{j : x_{j} !=0\\right\\}} W _{:,j} x_{j}+\\boldsymbol{b} \\\\ \\boldsymbol{p} &=\\operatorname{softmax}(\\boldsymbol{s}) \\\\ \\ell &=-\\log \\boldsymbol{p}_{e_{t}} \\end{aligned}\n",
        "$$\n",
        "\n",
        "We can get:\n",
        "\n",
        "$$\n",
        "\\begin{aligned} \\frac{d \\ell\\left(e_{t-n+1}^{t}, W, \\boldsymbol{b}\\right)}{d \\boldsymbol{b}} &=\\boldsymbol{p}-\\text { onehot }\\left(e_{t}\\right) \\\\ \\frac{d \\ell\\left(e_{t-n+1}^{t}, W, \\boldsymbol{b}\\right)}{d W_{\\cdot, j}} &=x_{j}\\left(\\boldsymbol{p}-\\text { onehot }\\left(e_{t}\\right)\\right) \\end{aligned}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "os16oqtXdIPy",
        "colab_type": "code",
        "outputId": "7f533d2c-f3cc-4ec4-db01-0ee4b983ec67",
        "colab": {}
      },
      "source": [
        "db = p - target_onehot\n",
        "dW = ((x >= 1.0).astype(int).reshape(-1, 1) * db).T\n",
        "print(db, b.shape, db.shape)\n",
        "print(dW, W.shape, dW.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 0.14884758  0.14884758 -0.8511524   0.40460965  0.14884758] (5,) (5,)\n",
            "[[ 0.          0.          0.          0.14884758  0.          0.\n",
            "   0.          0.14884758  0.          0.        ]\n",
            " [ 0.          0.          0.          0.14884758  0.          0.\n",
            "   0.          0.14884758  0.          0.        ]\n",
            " [-0.         -0.         -0.         -0.85115242 -0.         -0.\n",
            "  -0.         -0.85115242 -0.         -0.        ]\n",
            " [ 0.          0.          0.          0.40460965  0.          0.\n",
            "   0.          0.40460965  0.          0.        ]\n",
            " [ 0.          0.          0.          0.14884758  0.          0.\n",
            "   0.          0.14884758  0.          0.        ]] (5, 10) (5, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NyPDSKC8dIP3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sgd_opt_step(W, b, dW, db, lr=1e-4):\n",
        "  W -= lr * dW\n",
        "  b -= lr * db\n",
        "  return W, b"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XvFaG4pjdIP5",
        "colab_type": "code",
        "outputId": "cb55155f-1695-4a1a-c1af-886947edb759",
        "colab": {}
      },
      "source": [
        "# See how it changes\n",
        "print(b)\n",
        "W, b = sgd_opt_step(W, b, dW, db, lr=1e-4)\n",
        "print(b)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0. 0. 0. 1. 0.]\n",
            "[-1.4884758e-05 -1.4884758e-05  8.5115236e-05  9.9995953e-01\n",
            " -1.4884758e-05]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gd6aQ1iedIP7",
        "colab_type": "text"
      },
      "source": [
        "### Optimize our computation\n",
        "\n",
        "In fact, we are already optimize the computation in optimization steps.\n",
        "The things for our model is that we don't need to generate a one-hot vector for a feature.\n",
        "We only needs to pluck a column instead of dot product when we perform transformation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZCpzDkYcdIP8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def col_feature_fn(seq, vocab_size):\n",
        "  def gen():\n",
        "    acc = 0\n",
        "    for s in seq:\n",
        "      yield s + acc\n",
        "      acc += vocab_size\n",
        "  \n",
        "  return [f for f in gen()]\n",
        "\n",
        "def score(x, W, b):\n",
        "  # we don't have broadcast according to formula. It's different than engineering process\n",
        "  return np.sum(W[:,x], axis=1) + b"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YrxltfqPdIP-",
        "colab_type": "code",
        "outputId": "9cc09646-79d3-43c0-d1d2-476bd73e3010",
        "colab": {}
      },
      "source": [
        "x = col_feature_fn(seq, vocab_size)\n",
        "score(x, W, b)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-4.4654273e-05, -4.4654273e-05,  2.5534572e-04,  9.9987859e-01,\n",
              "       -4.4654273e-05], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TH4mNoN-dIQA",
        "colab_type": "text"
      },
      "source": [
        "## Putting them together\n",
        "Just show case which part might be really slow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c-qqnyEQdIQA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%load_ext line_profiler\n",
        "\n",
        "def step():\n",
        "  feature_length = 2\n",
        "  vocab_size = len(VOCABS)\n",
        "\n",
        "  seq = (1,2)\n",
        "  target = 3\n",
        "  W, b = build_model(vocab_size, feature_length) \n",
        "  x = col_feature_fn(seq, vocab_size)\n",
        "  s = score(x, W, b)\n",
        "  p = probability(s)\n",
        "  target_onehot = feature_fn((target,), vocab_size)\n",
        "  l = loss(p, target_onehot)\n",
        "  db = p - target_onehot\n",
        "  dW = np.zeros(W.shape[1], dtype=np.float32) # we spent a lot of time here though!\n",
        "  dW[np.array(x)] = 1.0\n",
        "  dW = (dW.reshape(-1, 1) * db).T\n",
        "  sgd_opt_step(W, b, dW, db, lr=1e-4)\n",
        "\n",
        "%lprun -f step step()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mqPlcLVKdIQC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pad_sequence(\n",
        "    sequence,\n",
        "    n,\n",
        "    mapping\n",
        "):\n",
        "  LEFT_PAD_SYMBOL = int(mapping['<s>'])\n",
        "  RIGHT_PAD_SYMBOL = int(mapping['</s>'])\n",
        "  return chain((LEFT_PAD_SYMBOL,) * (n - 1), iter(sequence), (RIGHT_PAD_SYMBOL,) * (n - 1))\n",
        "\n",
        "def ngram(sequence, n, mapping):\n",
        "  sequence = pad_sequence(sequence, n, mapping)\n",
        "\n",
        "  history = []\n",
        "  while n > 1:\n",
        "    try:\n",
        "      next_item = next(sequence)\n",
        "    except StopIteration:\n",
        "      # no more data, terminate the generator\n",
        "      return\n",
        "    history.append(next_item)\n",
        "    n -= 1\n",
        "  \n",
        "  for item in sequence:\n",
        "    history.append(mapping[item])\n",
        "    yield tuple(history)\n",
        "    del history[0]\n",
        "\n",
        "def train(filename, mapping, epoch=10):\n",
        "  vocab_size = len(VOCABS)\n",
        "  feature_length = 2\n",
        "  W, b = build_model(vocab_size, feature_length) \n",
        "  \n",
        "  with open(filename, 'r') as f:\n",
        "    for e in range(epoch):\n",
        "      dW = np.zeros(W.shape, np.float32)\n",
        "      db = np.zeros(b.shape, np.float32)\n",
        "      loss_val = 0.0\n",
        "      i = 0\n",
        "      \n",
        "      for l in f:\n",
        "        for seq in ngram(map(int, l.rstrip().split(' ')), feature_length+1, mapping):\n",
        "          x = col_feature_fn(seq[:-1], vocab_size)\n",
        "          s = score(x, W, b)\n",
        "          p = probability(s)\n",
        "\n",
        "          target_onehot = feature_fn((seq[-1],), vocab_size)\n",
        "          loss_val += loss(p, target_onehot)\n",
        "\n",
        "          db_val = p - target_onehot\n",
        "          db += db_val\n",
        "          dW[:,x] += (dW[:,x].T + db_val.T).T\n",
        "\n",
        "        i += 1\n",
        "        if i >= 100:\n",
        "          i = 0\n",
        "          print(f\"loss: {loss_val}\")\n",
        "          loss_val = 0.0\n",
        "          W, b = sgd_opt_step(W, b, dW, db, lr=1e-4)\n",
        "          dW = np.zeros(W.shape, np.float32)\n",
        "          db = np.zeros(b.shape, np.float32)\n",
        "      f.seek(0)\n",
        "  return W, b"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0FC3i3IedIQD",
        "colab_type": "code",
        "outputId": "d564dfc2-68cf-41c0-8da7-d7b2ce8d7964",
        "colab": {}
      },
      "source": [
        "W, b = train(\"data/loglinear/train.txt\", mapping)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:51: RuntimeWarning: overflow encountered in add\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "loss: 23338.241806030273\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/site-packages/scipy/special/_logsumexp.py:215: RuntimeWarning: invalid value encountered in subtract\n",
            "  return np.exp(x - logsumexp(x, axis=axis, keepdims=True))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "invalid literal for int() with base 10: ''",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-9be9cbe0b67b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data/loglinear/train.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-38-c0e9158b4d73>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(filename, mapping, epoch)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mseq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mngram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_length\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m           \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcol_feature_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m           \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-38-c0e9158b4d73>\u001b[0m in \u001b[0;36mngram\u001b[0;34m(sequence, n, mapping)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mn\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msequence\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32myield\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: ''"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JIZb2HhedIQG",
        "colab_type": "text"
      },
      "source": [
        "This is not efficient code at all. Since we take all data into generate vocabs,\n",
        "so we don't need to deal with unknown words. But if we want to make it more fruitful,\n",
        "we can generate vocabs based on training data (which is also correct).\n",
        "\n",
        "And be noted, this is not common practice to write computation like this.\n",
        "It's hard to vectorize. It's hard to use broadcast from numpy (which asks us to align the last dimension).\n",
        "Why is that? (Thing about memory layout)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ANFBBbCGdIQG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}